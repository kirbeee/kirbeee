---
id: Transformer
title: Transformer
last_update:
   date: 2025-07-18
   author: Kirbee
---
## Reference
- [Transformer Paper](https://arxiv.org/abs/1706.03762)

## Self-Attention
### Input and Output
- Input: Sequence of vectors (e.g., word embeddings, one-hot encoding)
- Output:
1. N -> model -> N  (e.g. POS tagging)
2. N -> model -> 1  (e.g. classification)
3. N -> model -> N' (e.g. translation)

### Relevant 
1. Dot-Product

2. Additive
### Self-Attention Mechanism
